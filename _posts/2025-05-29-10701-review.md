---
layout: post
title: Course review --- CMU 10701
date: 2025-05-29 20:00:00
description: Course review and reflection on CMU 10701, introduction to machine learning (PhD)
tag: ["computer science", "machine learning"]
categories: review
---

I was going to play some basketball in UW, but the court is taken by some club 
tournament. Therefore here I am writing this course review on CMU 10701, 
introduction to machine learning (PhD). As a comparison, I will mention another 
relevant course I self-studied --- Stanford CS229, also introduction to machine 
learning.

I self studied Stanford CS229 during Summer 2024. I think it took me at least 
half of summer to watch all the lectures (2018 version)
and complete all the assignments. I 
was also doing research at that time, even though that is a relatively 
stress-free hustle. I took CMU 10701 in Spring 2025, with instructors 
being Professor Geoffery Gordon and Professor 
Max Simchowitz. One thing I need to point out 
is that I registered for this class with time conflict. I did not attend 
any lectures except for the ones that are midterm and final. However there 
are lecture recordings so I watched some of them, or actually I think most 
of them.

Overall, I would rate the quality of the class about 6.5
out of 10. As a comparison, 
I would rate the quality of Stanford CS229 9.5 out of 10, again this is the 
2018 version of the course. Even though I don't really care 
who the professor is as I mentioned before, the teaching of this 
course, at least this semester, is horrible. Even though by no means am I 
trying to say the instructors do not know their stuff for machine learning, 
they are really not good teachers. Professor Gordon has been teaching 
this class for many years as far as I know, and he is 
also probably one of the oldest people in this school that has taken 10701. 
His lectures usually are a partially completed slides, while he fills it out 
using a tablet. The slide are by no means good, featuring long sentences 
that hardly grab attention. The handwritten part are also quite hard to 
understand. On the other hand, it's probably 
Professor Simchowitz's probably first time teaching any course. His lectures 
usually feature a slide with many typos, which he claims will be fixed 
after lecture and usually the typos are still there when I download them 
after two days of the lecture. He also has a huge ego, like to mention 
random terms and complicated concepts that are unnecessary. As a comparison, 
Stanford CS229 features Andrew Ng as the instructor. He always write on the 
board directly. The lectures are well organized and well structured, with 
natural mathematical calculations and providing deep insights into the topics.
I guess this is a combination of good lecture style and good course material, 
which is something I will talk about next.

The course material of CMU 10701 are good and bad at the same time. 
We shall compare the course material of both.
As expected
for an introduction to machine learning class, linear regression, logistic 
regression, kernels and SVMs, deep learning basics, PCA, and reinforcement 
learning basics are covered. There are quite a few notable differences. 
First for CMU 10701, it covers convolutional networks (CNN), recurrent 
neural networks (RNN), attentions 
and transformers in LECTURE. Considering the fact that the Stanford class 
I watched is 2018, we cannot really say much about this. CMU 10701 also covers 
some basic learning theory (PAC learning and VC dimension), 
and some introduction to graphical models. 
These are not covered by the Stanford 
class, and that's why I say the course material is good. The CNN, RNN, 
attention and transformers class are no doubt great introduction 
to more advanced deep learning class, and classes that focus more on computer 
vision or natural language processing. The graphical models part also 
improves my feelings toward the class by a lot, as I like probability and 
things on the theory side. Therefore I find this really interesting
We must note here though by "course material is good", I do not mean 
"their presentation of this material is good". It's more like it gives me 
a chance to learn more about these by myself, which turns out to be really 
interesting. The only exception of this is the one assignment that requires us 
to implement RNN and transformer encoder using PyTorch. I learned PyTorch 
from this, and the implementation process improves my understanding of these. 
However all the materials I referenced while during the homework is either 
slides for Stanford CS224n (Natural Language Processing), or the paper 
*Attention is all you need* itself, instead of the lecture slides. 
For graphical models, the assignment again help me understand variable 
elimnation better, but the real click is when I read Bishop's Patter Recognition
and Machine Learning (PRML) book. 
It gives a more structured introduction to hidden 
Markov models and graphical models, while generalizing the stuff covered 
in class into the stage where I feel very interested in. That's also 
partially why I will take 
10708 probabilistic graphical models next semester. I hope it will give me 
the chance of reading more of this book.
Without this transformer homework and the graphical model part of the course
I would probably just give a 5 out of 10 for this class.

Now why is the course material bad?
Seemingly the Stanford CS229 covers less material, but I think the 
organization of material is much better. Let's see an example. Both class 
covered this thing called the Variational Auto-encoder (VAE) and 
some basics of diffusion models, in the 
unsupervised learning section. In 10701, this seems like a very separated
lecture called "generative AI", after talking about contrastive learning 
and PCA which I don't think is quite relevant to this. In CS229 this is 
not in the lectures but in the notes as an optional material, but the way 
this get introduced is much more natural. For unsupervised learning, CS229 
talked about k-means, PCA, ICA. But the difference maker is this thing 
called the mixture of Gaussians and Expectation Maximization (EM). In my 
opinion EM is the single most important thing for the theory of 
probabilistic methods, and 
VAE precisely utilized the generalized EM algorithm. Generalized EM is also 
everywhere in PRML. Relevant to this is some lower bound of a distribution 
called ELBO. CS229 introduces this ELBO through Jensen's inequality, and take
this to a further step through KL divergence in homework. In comparison, 
10701 pulls this lower bound out of nowhere 
using a Taylor's expansion in a weird 
form hidden in Professor Gordon's squibble. This in my opinion is very 
unnatural, and the funniest part is they put the KL divergence understanding
on the finals. In my opinion EM is the key of understand VAE and diffusion 
models, and I am pretty sure minimal amount of people actually understand
what is covered in the generative AI lecture, and surprise surprise this 
section contribute to another huge part of the final. 

The RL section is another perfect example for this. Seemingly both classes 
covered basics of RL, but 10701 only covers the dynamic programming approach 
to finite horizon MDP. In contrast, CS229 introduces value interation and 
policy interation using Bellman equation, and in homework we PROVED Bellman 
update oprator that are used in value iteration is a CONTRACTION MAP! If 
this is unfamiliar to you, this mean there is a fixed point in the space, 
and in this case the optimal policy. This gives the correctness of 
the algorithms and I personally think this is very exciting. DP approach 
for finite horizon MDP is of course also covered and there are so much more 
in the notes. Therefore I cannot help but think --- why can't 10701 reduces 
the introduction of unnecessary random terms, but actually covers the 
content more in depth? Similarly, topics like contrastive learning 
simply doesn't fit into this intro course, but it ended up taking another huge 
part of the final. 

One other difference of the two courses is the homework. CS229 only allows 
NumPy, no PyTorch. I don't think this is better or worse, but I think 
this shows the more emphasis on actually understanding 
the mathematics behind every algorithm, and in the end we implemented 
a simple CNN for handwritten digit classification, using only NumPy including 
the back propogation. However again I like transformer part of homework in 
10701, as I already said. Also I am not definitely complaining learning 
PyTorch, which is definitely something I should know, through this chance.

I guess that's it for my comment on these two classes. Overall I think 
CS229 gives a much better foundational understanding for these 
introductory machine learning concepts, the homework also gives 
insights on both the math and the hands-on. The math part of CS229 definitely 
stands out much more through both lectures and homework. For a side 
note, I cannot give any 
advice on the selection between 10301 and 10701, because I have never 
taken 10301. I also never considered it because before this semester 
10301 does not count for major and I want to use 10701 as a prerequisite
for a lot more graduate machine learning class.

